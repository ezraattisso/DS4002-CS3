{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nAV4QFHr1m7"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummary\n",
        "\n",
        "import os                       # for working with files\n",
        "import numpy as np              # for numerical computationss\n",
        "import pandas as pd             # for working with dataframes\n",
        "import torch                    # Pytorch module\n",
        "import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\n",
        "import torch.nn as nn           # for creating  neural networks\n",
        "from torch.utils.data import DataLoader # for dataloaders\n",
        "from PIL import Image           # for checking images\n",
        "import torch.nn.functional as F # for functions for calculating loss\n",
        "import torchvision.transforms as transforms   # for transforming images into tensors\n",
        "from torchvision.utils import make_grid       # for data checking\n",
        "from torchvision.datasets import ImageFolder  # for working with classes and images\n",
        "from torchsummary import summary              # for getting the summary of our model\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## to upload test zip file.\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_name = \"/content/test.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done.')\n",
        "\n",
        "\n",
        "## to upload train zip file.\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_name = \"/content/train.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done again.')\n",
        "\n",
        "# setting up our data dictionaries.\n",
        "\n",
        "# data_dir = \"/content/PROJ3\"\n",
        "# train_dir = data_dir + \"/Train\"\n",
        "test_dir = \"/content/test\"\n",
        "train_dir = \"/content/train\"\n",
        "#test_dir = data_dir + \"/Test\"\n",
        "\n",
        "diseases = os.listdir(train_dir)\n",
        "\n",
        "print(diseases)\n",
        "\n",
        "\n",
        "indices = [1, 2, 3, 4, 7, 9, 10, 11, 14, 15, 16, 19, 20, 21, 24]\n",
        "\n",
        "# Assign the list comprehension result to new_diseases\n",
        "new_diseases = [diseases[i] for i in indices]\n",
        "\n",
        "\n",
        "# Print the new_diseases list\n",
        "print(new_diseases)\n",
        "print(\"Total disease classes are: {}\".format(len(diseases)))\n",
        "\n",
        "plants = []\n",
        "NumberOfDiseases = 0\n",
        "for plant in new_diseases:\n",
        "    parts = plant.split('___')  # Split the plant string\n",
        "    if len(parts) > 0 and parts[0] not in plants: # Check if there's at least one element before accessing it\n",
        "        plants.append(parts[0])\n",
        "    if len(parts) > 1 and parts[1] != 'healthy': # Check if there's a second element before accessing it\n",
        "        NumberOfDiseases += 1\n",
        "\n",
        "\n",
        "# unique plants in the dataset\n",
        "print(f\"Unique Plants are: \\n{plants}\")\n",
        "\n",
        "print(\"Number of plants: {}\".format(len(plants)))\n",
        "\n",
        "print(\"Number of diseases: {}\".format(NumberOfDiseases))\n",
        "\n",
        "# Number of images for each disease\n",
        "nums = {}\n",
        "for disease in diseases:\n",
        "    disease_path = os.path.join(train_dir, disease)\n",
        "    if os.path.isdir(disease_path):  # Checking if it's actually a directory (had some issues with DStore metadata folder being added)\n",
        "        nums[disease] = len(os.listdir(disease_path))\n",
        "\n",
        "# converting the nums dictionary to pandas dataframe\n",
        "img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"no. of images\"])\n",
        "img_per_class\n",
        "\n",
        "\n",
        "# plotting number of images available for each disease\n",
        "# index = [n for n in range(38)] # This creates a list of 38 elements\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.bar(nums.keys(), nums.values(), width=0.3) # Use keys and values directly\n",
        "plt.xlabel('Plants/Diseases', fontsize=10)\n",
        "plt.ylabel('No of images available', fontsize=10)\n",
        "plt.xticks(range(len(nums)), nums.keys(), fontsize=5, rotation=90) # Use range based on num keys\n",
        "plt.title('Images per each class of plant disease')\n",
        "\n",
        "\n",
        "n_train = 0\n",
        "for value in nums.values():\n",
        "    n_train += value\n",
        "print(f\"There are {n_train} images for training.\")\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define a transform that resizes all images to the same dimensions\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Choose dimensions that work for your model\n",
        "    transforms.ToTensor(),\n",
        "    # Add any other transformations you need\n",
        "])\n",
        "\n",
        "test = ImageFolder(test_dir, transform=transform) # Apply transform here\n",
        "train = ImageFolder(train_dir, transform=transform) # Apply transform here\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove the .ipynb_checkpoints directory\n",
        "checkpoint_dir = os.path.join(test_dir, '.ipynb_checkpoints')\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "checkpoint_dir = os.path.join(train_dir, '.ipynb_checkpoints')\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    shutil.rmtree(checkpoint_dir)\n",
        "\n",
        "\n",
        "import cv2\n",
        "\n",
        "img_train = []\n",
        "label_train = []\n",
        "\n",
        "directory = \"/content/train\"\n",
        "\n",
        "for crop_folder in os.listdir(directory):\n",
        "    # Skip .DS_Store file\n",
        "    if crop_folder == \".DS_Store\":\n",
        "        continue\n",
        "\n",
        "    sub_direct = os.path.join(directory, crop_folder)\n",
        "    crop_type = crop_folder.split()[0]\n",
        "\n",
        "    for filename in os.listdir(sub_direct):\n",
        "        file = os.path.join(sub_direct, filename)\n",
        "        if os.path.isfile(file):\n",
        "          img = cv2.imread(file)\n",
        "        # Resize image to (128, 128)\n",
        "        try:\n",
        "            img_resized = cv2.resize(img, (1400, 1000), interpolation=cv2.INTER_AREA)\n",
        "        except:\n",
        "            print(file, \" - failed\")\n",
        "        # Convert image to grayscale\n",
        "        img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
        "        img_train.append(img_gray)\n",
        "        img_train.append(file)\n",
        "        label_train.append(crop_type)\n",
        "        if os.path.isdir(disease_path):  # Checking if it's actually a directory (had some issues with DStore metadata folder being added)\n",
        "          nums[disease] = len(os.listdir(disease_path))\n",
        "\n",
        "\n",
        "img, label = train[0]\n",
        "print(img.shape, label)\n",
        "\n",
        "# for checking some images from training dataset\n",
        "def show_image(image, label):\n",
        "    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "\n",
        "\n",
        "\n",
        "show_image(*train[130]) # dislaying what a healthy leaf would look like.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DsTal0nFr96r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual model building - I won't give you all the code, but I will set you up with some prelimary code to give you some ideas on how to proceed..."
      ],
      "metadata": {
        "id": "JTEk1DeHtN1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the seed value.\n",
        "random_seed = 7\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# setting the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# DataLoaders for training and validation\n",
        "train_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_dl = DataLoader(test, batch_size, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# helper function to show a batch of training instances\n",
        "def show_batch(data):\n",
        "    for images, labels in data:\n",
        "        # Resize images to a common size\n",
        "        resized_images = [transforms.Resize((224, 224))(img) for img in images]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(30, 30))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(resized_images, nrow=8).permute(1, 2, 0))\n",
        "        break\n",
        "\n",
        "## can change the 2 to match the number of cores that we're using.\n",
        "\n",
        "\n",
        "show_batch(train_dl)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "\n",
        "def get_available_cores():\n",
        "    \"\"\" Detect the number of available CPU cores on Rivanna. Returns # of cores allocated to the job or available on the system. \"\"\"\n",
        "    # First check SLURM environment variables (if running as a job)\n",
        "    if 'SLURM_CPUS_PER_TASK' in os.environ:\n",
        "        return int(os.environ['SLURM_CPUS_PER_TASK'])\n",
        "    elif 'SLURM_NTASKS' in os.environ:\n",
        "        return int(os.environ['SLURM_NTASKS'])\n",
        "\n",
        "    # If not in SLURM or as fallback, use multiprocessing\n",
        "    return multiprocessing.cpu_count()\n",
        "\n",
        "def parallel_process(function, items_list, n_jobs=None, verbose=1):\n",
        "    if n_jobs is None:\n",
        "        n_jobs = get_available_cores()\n",
        "\n",
        "    print(f\"Running {len(items_list)} tasks on {n_jobs} cores...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
        "        delayed(function)(item) for item in items_list\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def submit_slurm_job(script_path, job_name, n_cores=4, mem_per_cpu=\"4G\",\n",
        "                     time_limit=\"02:00:00\", partition=\"standard\",\n",
        "                     output_file=\"slurm_%j.out\"):\n",
        "    cmd = [\n",
        "        \"sbatch\",\n",
        "        f\"--job-name={job_name}\",\n",
        "        f\"--cpus-per-task={n_cores}\",\n",
        "        f\"--mem-per-cpu={mem_per_cpu}\",\n",
        "        f\"--time={time_limit}\",\n",
        "        f\"--partition={partition}\",\n",
        "        f\"--output={output_file}\",\n",
        "        script_path\n",
        "    ]\n",
        "\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        # Extract job ID from output (typically \"Submitted batch job 123456\")\n",
        "        job_id = result.stdout.strip().split()[-1]\n",
        "        print(f\"Job submitted with ID: {job_id}\")\n",
        "        return job_id\n",
        "    else:\n",
        "        print(f\"Error submitting job: {result.stderr}\")\n",
        "        return None\n",
        "\n",
        "def batch_process_with_numpy(data, processing_func, batch_size=1000):\n",
        "    results = []\n",
        "    total_items = len(data)\n",
        "\n",
        "    for i in range(0, total_items, batch_size):\n",
        "        batch = data[i:min(i + batch_size, total_items)]\n",
        "        batch_result = processing_func(batch)\n",
        "        results.append(batch_result)\n",
        "\n",
        "        print(f\"Processed batch {i//batch_size + 1}/{(total_items-1)//batch_size + 1}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_slurm_script(script_file, commands, modules=None, conda_env=None):\n",
        "    with open(script_file, 'w') as f:\n",
        "        f.write(\"#!/bin/bash\\n\\n\")\n",
        "\n",
        "        if modules:\n",
        "            for module in modules:\n",
        "                f.write(f\"module load {module}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        if conda_env:\n",
        "            f.write(f\"source activate {conda_env}\\n\\n\")\n",
        "\n",
        "        for cmd in commands:\n",
        "            f.write(f\"{cmd}\\n\")\n",
        "\n",
        "    # Make the script executable\n",
        "    os.chmod(script_file, 0o755)\n",
        "    print(f\"Created SLURM script: {script_file}\")\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "\n",
        "\n",
        "class RobustImageFolder(ImageFolder):\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            return super().__getitem__(idx)\n",
        "        except (UnidentifiedImageError, OSError) as e:\n",
        "            print(f\"Error loading image at index {idx}: {e}\")\n",
        "            if len(self) > 1:\n",
        "                new_idx = (idx + 1) % len(self)\n",
        "                print(f\"Trying alternate image at index {new_idx}\")\n",
        "                return self.__getitem__(new_idx)\n",
        "            else:\n",
        "                print(\"Creating blank placeholder image\")\n",
        "                placeholder = torch.zeros((3, 224, 224))\n",
        "                return placeholder, 0\n",
        "\n",
        "class TransformedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        if hasattr(dataset, 'targets'):\n",
        "            self.labels = dataset.targets\n",
        "        elif hasattr(dataset, 'labels'):\n",
        "            self.labels = dataset.labels\n",
        "        else:\n",
        "            self.labels = [s[1] for s in dataset.samples] if hasattr(dataset, 'samples') else [0] * len(dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Handle different dataset formats\n",
        "            if hasattr(self.dataset, 'samples'):\n",
        "                img_path, label = self.dataset.samples[idx]\n",
        "            elif isinstance(self.dataset[idx], tuple):\n",
        "                img_path, label = self.dataset[idx]\n",
        "            else:\n",
        "                img_path = self.dataset[idx]\n",
        "                label = self.labels[idx]\n",
        "\n",
        "            # Load the image\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # Apply transforms\n",
        "            if self.transform:\n",
        "                img = self.transform(img)  # This should include ToTensor()\n",
        "\n",
        "            return img, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a zero tensor with the correct dimensions\n",
        "            img = torch.zeros((3, 224, 224))\n",
        "            return img, self.labels[idx] if idx < len(self.labels) else 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "S2SA46FUs7jL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}